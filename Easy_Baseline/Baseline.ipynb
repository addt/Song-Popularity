{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape\n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2991289856021824869\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10966958080\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9771279223605662646\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sayan/workspace/Song-Popularity/data/fma_small\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "print(AUDIO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106574, 52), (106574, 518), (13129, 249))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = utils.load('../data/tracks.csv')\n",
    "features = utils.load('../data/features.csv')\n",
    "echonest = utils.load('../data/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()\n",
    "\n",
    "tracks.shape, features.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combo_df = pd.read_csv(\"../data/combined_features.csv\") \n",
    "mod_tracks = pd.read_csv(\"../data/mod_tracks.csv\")\n",
    "mod_features = pd.read_csv(\"../data/mod_features.csv\")\n",
    "mod_echonest = pd.read_csv(\"../data/mod_echonest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "mod_tracks[\"popularity_index\"] = pca.fit_transform(mod_tracks[[\"track_listens\",\"track_favorites\"]].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough Echonest features: (13129, 767)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8000, 52), (8000, 518))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = tracks.index[tracks['set', 'subset'] <= 'small']\n",
    "\n",
    "assert subset.isin(tracks.index).all()\n",
    "assert subset.isin(features.index).all()\n",
    "\n",
    "features_all = features.join(echonest, how='inner').sort_index(axis=1)\n",
    "print('Not enough Echonest features: {}'.format(features_all.shape))\n",
    "\n",
    "tracks = tracks.loc[subset]\n",
    "features_all = features.loc[subset]\n",
    "\n",
    "tracks.shape, features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 training examples, 800 validation examples, 800 testing examples\n",
      "Top genres (22): ['-', 'E', 'F', 'H', 'I', 'P', 'R', 'a', 'c', 'e', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'x']\n",
      "All genres (114): [1, 2, 6, 10, 12, 15, 16, 17, 18, 21, 22, 25, 26, 27, 30, 31, 32, 33, 36, 38, 41, 42, 45, 46, 47, 49, 53, 58, 64, 66, 70, 71, 76, 77, 79, 81, 83, 85, 86, 88, 89, 90, 92, 94, 98, 100, 101, 102, 103, 107, 109, 111, 113, 117, 118, 125, 130, 167, 171, 172, 174, 177, 180, 181, 182, 183, 184, 185, 186, 214, 224, 232, 236, 240, 247, 250, 267, 286, 296, 297, 314, 337, 359, 360, 361, 362, 400, 401, 404, 439, 440, 456, 468, 491, 495, 502, 504, 514, 524, 538, 539, 542, 580, 602, 619, 695, 741, 763, 808, 811, 1032, 1060, 1193, 1235]\n"
     ]
    }
   ],
   "source": [
    "train = tracks.index[tracks['set', 'split'] == 'training']\n",
    "val = tracks.index[tracks['set', 'split'] == 'validation']\n",
    "test = tracks.index[tracks['set', 'split'] == 'test']\n",
    "\n",
    "print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train, val, test])))\n",
    "\n",
    "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genre_top']).classes_)\n",
    "#genres = list(tracks['track', 'genre_top'].unique())\n",
    "print('Top genres ({}): {}'.format(len(genres), genres))\n",
    "genres = list(MultiLabelBinarizer().fit(tracks['track', 'genres_all']).classes_)\n",
    "print('All genres ({}): {}'.format(len(genres), genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    106574.000000\n",
      "mean       2329.353548\n",
      "std        8028.070647\n",
      "min           0.000000\n",
      "25%         292.000000\n",
      "50%         764.000000\n",
      "75%        2018.000000\n",
      "max      543252.000000\n",
      "Name: track_listens, dtype: float64\n",
      "count     17519.000000\n",
      "mean       7611.599143\n",
      "std       17884.315777\n",
      "min         800.642731\n",
      "25%        1749.650713\n",
      "50%        3381.640331\n",
      "75%        7196.655876\n",
      "max      540924.160149\n",
      "Name: popularity_index, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3435c19b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAELCAYAAAAVwss1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3435c11d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mod_tracks[\"track_listens\"].describe())\n",
    "print(mod_tracks[mod_tracks[\"popularity_index\"] >= 800][\"popularity_index\"].describe())\n",
    "sns.distplot(mod_tracks[mod_tracks[\"track_listens\"] >= 100000][\"track_listens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "(8000, 1)\n"
     ]
    }
   ],
   "source": [
    "listens_label = mod_tracks[\"track_listens\"][subset].map(lambda x: 1 if x > 800 else 0)\n",
    "print(listens_label.unique())\n",
    "listens_label = pd.DataFrame(listens_label, index=tracks.index)\n",
    "print(listens_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listens_label.track_listens.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1321967)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just be sure that everything is fine. Multiprocessing is tricky to debug.\n",
    "utils.FfmpegLoader().load(utils.get_audio_path(AUDIO_DIR, 2))\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, listens_label, utils.FfmpegLoader())\n",
    "SampleLoader(train, batch_size=2).__next__()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras parameters.\n",
    "# NB_WORKER = len(os.sched_getaffinity(0))  # number of usables CPUs\n",
    "# NB_WORKER = 8 # len(os.sched_getaffinity(0))  # number of usables CPUs\n",
    "params = {'pickle_safe': False, 'max_q_size': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "merged = concatenate([x1, y1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality: (59953,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              59954000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 60,054,201\n",
      "Trainable params: 60,054,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      " 301/6400 [>.............................] - ETA: 20:24 - loss: 11.0167 - acc: 0.309098567\n",
      " 369/6400 [>.............................] - ETA: 20:10 - loss: 11.0171 - acc: 0.308998565\n",
      " 711/6400 [==>...........................] - ETA: 19:01 - loss: 11.0319 - acc: 0.308055783\n",
      " 815/6400 [==>...........................] - ETA: 18:40 - loss: 11.1303 - acc: 0.301854576\n",
      "2093/6400 [========>.....................] - ETA: 14:25 - loss: 11.0447 - acc: 0.307298569\n",
      "2135/6400 [=========>....................] - ETA: 14:16 - loss: 11.0439 - acc: 0.3073133297\n",
      "2433/6400 [==========>...................] - ETA: 13:16 - loss: 11.1197 - acc: 0.302599134\n",
      "3002/6400 [=============>................] - ETA: 11:22 - loss: 11.1416 - acc: 0.3011108925\n",
      "3153/6400 [=============>................] - ETA: 10:51 - loss: 11.1946 - acc: 0.297854578\n",
      "4021/6400 [=================>............] - ETA: 7:57 - loss: 11.2045 - acc: 0.297217637\n",
      "4833/6400 [=====================>........] - ETA: 5:14 - loss: 11.2385 - acc: 0.295117634\n",
      "4920/6400 [======================>.......] - ETA: 4:57 - loss: 11.2569 - acc: 0.293917631\n",
      "5860/6400 [==========================>...] - ETA: 1:48 - loss: 11.2712 - acc: 0.293017635\n",
      "6136/6400 [===========================>..] - ETA: 53s - loss: 11.2865 - acc: 0.292017636\n",
      "6400/6400 [==============================] - 1285s 201ms/step - loss: 11.2892 - acc: 0.2919\n",
      "Epoch 2/5\n",
      "  88/6400 [..............................] - ETA: 21:10 - loss: 11.4133 - acc: 0.284117636\n",
      " 307/6400 [>.............................] - ETA: 20:23 - loss: 10.9052 - acc: 0.316098565\n",
      " 495/6400 [=>............................] - ETA: 19:45 - loss: 11.0147 - acc: 0.309198567\n",
      " 573/6400 [=>............................] - ETA: 19:28 - loss: 11.0456 - acc: 0.3072133297\n",
      " 647/6400 [==>...........................] - ETA: 19:13 - loss: 11.0882 - acc: 0.304598569\n",
      " 668/6400 [==>...........................] - ETA: 19:08 - loss: 11.1931 - acc: 0.297917637\n",
      "2975/6400 [============>.................] - ETA: 11:28 - loss: 11.2856 - acc: 0.292154578\n",
      "3076/6400 [=============>................] - ETA: 11:07 - loss: 11.2778 - acc: 0.292699134\n",
      "3129/6400 [=============>................] - ETA: 10:57 - loss: 11.2753 - acc: 0.292717634\n",
      "3907/6400 [=================>............] - ETA: 8:21 - loss: 11.2907 - acc: 0.291854576\n",
      "4129/6400 [==================>...........] - ETA: 7:36 - loss: 11.2898 - acc: 0.2918108925\n",
      "4590/6400 [====================>.........] - ETA: 6:03 - loss: 11.2778 - acc: 0.292617635\n",
      "5428/6400 [========================>.....] - ETA: 3:15 - loss: 11.3106 - acc: 0.290517631\n",
      "6194/6400 [============================>.] - ETA: 41s - loss: 11.2709 - acc: 0.293055783\n",
      "6400/6400 [==============================] - 1287s 201ms/step - loss: 11.2867 - acc: 0.2920\n",
      "Epoch 3/5\n",
      " 103/6400 [..............................] - ETA: 20:58 - loss: 11.4538 - acc: 0.281698569\n",
      " 201/6400 [..............................] - ETA: 20:41 - loss: 11.1835 - acc: 0.2985133297\n",
      "1240/6400 [====>.........................] - ETA: 17:16 - loss: 11.1982 - acc: 0.297699134\n",
      "1537/6400 [======>.......................] - ETA: 16:16 - loss: 11.1296 - acc: 0.301998567\n",
      "2261/6400 [=========>....................] - ETA: 13:51 - loss: 11.3240 - acc: 0.289717636\n",
      "2723/6400 [===========>..................] - ETA: 12:18 - loss: 11.4108 - acc: 0.284217634\n",
      "3389/6400 [==============>...............] - ETA: 10:04 - loss: 11.4170 - acc: 0.283917637\n",
      "3902/6400 [=================>............] - ETA: 8:22 - loss: 11.4195 - acc: 0.2837108925\n",
      "4034/6400 [=================>............] - ETA: 7:55 - loss: 11.4292 - acc: 0.283117631\n",
      "4076/6400 [==================>...........] - ETA: 7:47 - loss: 11.4288 - acc: 0.283117635\n",
      "4438/6400 [===================>..........] - ETA: 6:34 - loss: 11.4054 - acc: 0.284654578\n",
      "5877/6400 [==========================>...] - ETA: 1:45 - loss: 11.3227 - acc: 0.289855783\n",
      "5912/6400 [==========================>...] - ETA: 1:38 - loss: 11.3177 - acc: 0.290198565\n",
      "5974/6400 [===========================>..] - ETA: 1:25 - loss: 11.3070 - acc: 0.290854576\n",
      "6400/6400 [==============================] - 1288s 201ms/step - loss: 11.2867 - acc: 0.2920\n",
      "Epoch 4/5\n",
      " 150/6400 [..............................] - ETA: 20:58 - loss: 10.4157 - acc: 0.346798567\n",
      " 770/6400 [==>...........................] - ETA: 18:56 - loss: 10.9112 - acc: 0.315698569\n",
      "1036/6400 [===>..........................] - ETA: 18:02 - loss: 11.0027 - acc: 0.309854578\n",
      "1945/6400 [========>.....................] - ETA: 14:59 - loss: 11.2785 - acc: 0.292517634\n",
      "2288/6400 [=========>....................] - ETA: 13:50 - loss: 11.3227 - acc: 0.289817637\n",
      "2391/6400 [==========>...................] - ETA: 13:29 - loss: 11.3084 - acc: 0.290754576\n",
      "3433/6400 [===============>..............] - ETA: 9:59 - loss: 11.3960 - acc: 0.285217636\n",
      "3526/6400 [===============>..............] - ETA: 9:40 - loss: 11.3939 - acc: 0.285355783\n",
      "3909/6400 [=================>............] - ETA: 8:22 - loss: 11.3746 - acc: 0.286599134\n",
      "3920/6400 [=================>............] - ETA: 8:20 - loss: 11.3834 - acc: 0.286017631\n",
      "4279/6400 [===================>..........] - ETA: 7:08 - loss: 11.3486 - acc: 0.288217635\n",
      "4478/6400 [===================>..........] - ETA: 6:27 - loss: 11.3355 - acc: 0.289098565\n",
      "5433/6400 [========================>.....] - ETA: 3:15 - loss: 11.2914 - acc: 0.2917108925\n",
      "5685/6400 [=========================>....] - ETA: 2:24 - loss: 11.3041 - acc: 0.2909133297\n",
      "6400/6400 [==============================] - 1291s 202ms/step - loss: 11.2992 - acc: 0.2913\n",
      "Epoch 5/5\n",
      " 146/6400 [..............................] - ETA: 21:05 - loss: 12.0114 - acc: 0.2466133297\n",
      "1244/6400 [====>.........................] - ETA: 17:21 - loss: 11.5595 - acc: 0.2749108925\n",
      "1532/6400 [======>.......................] - ETA: 16:22 - loss: 11.7070 - acc: 0.265798569\n",
      "1938/6400 [========>.....................] - ETA: 15:00 - loss: 11.6730 - acc: 0.267854576\n",
      "3029/6400 [=============>................] - ETA: 11:20 - loss: 11.6160 - acc: 0.271417637\n",
      "3335/6400 [==============>...............] - ETA: 10:18 - loss: 11.7070 - acc: 0.265754578\n",
      "3618/6400 [===============>..............] - ETA: 9:21 - loss: 11.6065 - acc: 0.272017631\n",
      "3845/6400 [=================>............] - ETA: 8:35 - loss: 11.5639 - acc: 0.274655783\n",
      "4280/6400 [===================>..........] - ETA: 7:07 - loss: 11.4875 - acc: 0.279498565\n",
      "4686/6400 [====================>.........] - ETA: 5:45 - loss: 11.4210 - acc: 0.283617635\n",
      "5296/6400 [=======================>......] - ETA: 3:42 - loss: 11.3246 - acc: 0.289799134\n",
      "5910/6400 [==========================>...] - ETA: 1:38 - loss: 11.3215 - acc: 0.289817636\n",
      "6259/6400 [============================>.] - ETA: 28s - loss: 11.3143 - acc: 0.290398567\n",
      "6319/6400 [============================>.] - ETA: 16s - loss: 11.3078 - acc: 0.290717634\n",
      "6400/6400 [==============================] - 1291s 202ms/step - loss: 11.2942 - acc: 0.2916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.299165204912438, 0.29125000000000001]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = utils.FfmpegLoader(sampling_rate=2000)\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, listens_label, loader)\n",
    "print('Dimensionality: {}'.format(loader.shape))\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(units=1000, input_shape=loader.shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(units=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(units=listens_label.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# optimizer = keras.optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit_generator(SampleLoader(train, batch_size=1), train.size, epochs=5)\n",
    "    loss = model.evaluate_generator(SampleLoader(val, batch_size=1), val.size)\n",
    "    loss = model.evaluate_generator(SampleLoader(test, batch_size=1), test.size)\n",
    "    Y = model.predict_generator(SampleLoader(test, batch_size=1), test.size);\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = SampleLoader(train, batch_size=32)\n",
    "print(a, type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 479625, 1)\n",
      "(None, 936, 128)\n",
      "(None, 929, 32)\n",
      "(None, 225, 32)\n",
      "(None, 56, 32)\n",
      "(None, 1792)\n",
      "(None, 100)\n",
      "(None, 1)\n",
      "Epoch 1/5\n",
      " 273/6400 [>.............................] - ETA: 15:17 - loss: 11.2706 - acc: 0.293054576\n",
      "2483/6400 [==========>...................] - ETA: 9:46 - loss: 11.4480 - acc: 0.281998567\n",
      "2946/6400 [============>.................] - ETA: 8:37 - loss: 11.4508 - acc: 0.281799134\n",
      "3748/6400 [================>.............] - ETA: 6:37 - loss: 11.3443 - acc: 0.288417634\n",
      "3970/6400 [=================>............] - ETA: 6:03 - loss: 11.3404 - acc: 0.288754578\n",
      "4056/6400 [==================>...........] - ETA: 5:51 - loss: 11.3397 - acc: 0.288755783\n",
      "4089/6400 [==================>...........] - ETA: 5:46 - loss: 11.3145 - acc: 0.290398565\n",
      "4607/6400 [====================>.........] - ETA: 4:28 - loss: 11.3157 - acc: 0.290217631\n",
      "5305/6400 [=======================>......] - ETA: 2:43 - loss: 11.2363 - acc: 0.2952108925\n",
      "5768/6400 [==========================>...] - ETA: 1:34 - loss: 11.2658 - acc: 0.293317636\n",
      "5796/6400 [==========================>...] - ETA: 1:30 - loss: 11.2609 - acc: 0.293798569\n",
      "5895/6400 [==========================>...] - ETA: 1:15 - loss: 11.2530 - acc: 0.2941133297\n",
      "6348/6400 [============================>.] - ETA: 7s - loss: 11.2812 - acc: 0.292417635\n",
      "6351/6400 [============================>.] - ETA: 7s - loss: 11.2834 - acc: 0.292217637\n",
      "6400/6400 [==============================] - 958s 150ms/step - loss: 11.2917 - acc: 0.2917\n",
      "Epoch 2/5\n",
      " 900/6400 [===>..........................] - ETA: 13:42 - loss: 11.4077 - acc: 0.284454576\n",
      "2454/6400 [==========>...................] - ETA: 9:50 - loss: 11.3104 - acc: 0.2905133297\n",
      "3367/6400 [==============>...............] - ETA: 7:33 - loss: 11.3874 - acc: 0.285755783\n",
      "3626/6400 [===============>..............] - ETA: 6:55 - loss: 11.3698 - acc: 0.286817631\n",
      "3688/6400 [================>.............] - ETA: 6:45 - loss: 11.3862 - acc: 0.285898565\n",
      "3881/6400 [=================>............] - ETA: 6:16 - loss: 11.3663 - acc: 0.287017636\n",
      "4394/6400 [===================>..........] - ETA: 5:00 - loss: 11.2765 - acc: 0.292798567\n",
      "4614/6400 [====================>.........] - ETA: 4:27 - loss: 11.2744 - acc: 0.292898569\n",
      "4725/6400 [=====================>........] - ETA: 4:10 - loss: 11.3132 - acc: 0.290417637\n",
      "4770/6400 [=====================>........] - ETA: 4:03 - loss: 11.3101 - acc: 0.290617635\n",
      "4950/6400 [======================>.......] - ETA: 3:36 - loss: 11.3143 - acc: 0.290354578\n",
      "5080/6400 [======================>.......] - ETA: 3:17 - loss: 11.3386 - acc: 0.2888108925\n",
      "5173/6400 [=======================>......] - ETA: 3:03 - loss: 11.3319 - acc: 0.289299134\n",
      "5716/6400 [=========================>....] - ETA: 1:42 - loss: 11.3097 - acc: 0.290617634\n",
      "6400/6400 [==============================] - 958s 150ms/step - loss: 11.2892 - acc: 0.2919\n",
      "Epoch 3/5\n",
      " 250/6400 [>.............................] - ETA: 15:23 - loss: 11.4147 - acc: 0.284055783\n",
      " 441/6400 [=>............................] - ETA: 14:53 - loss: 11.5682 - acc: 0.274498567\n",
      " 699/6400 [==>...........................] - ETA: 14:13 - loss: 11.3581 - acc: 0.2876108925\n",
      "1682/6400 [======>.......................] - ETA: 11:45 - loss: 11.1748 - acc: 0.299017635\n",
      "2384/6400 [==========>...................] - ETA: 10:00 - loss: 11.3750 - acc: 0.286517631\n",
      "2654/6400 [===========>..................] - ETA: 9:20 - loss: 11.3471 - acc: 0.288217634\n",
      "2668/6400 [===========>..................] - ETA: 9:18 - loss: 11.3592 - acc: 0.2875133297\n",
      "4015/6400 [=================>............] - ETA: 5:56 - loss: 11.3125 - acc: 0.290417636\n",
      "4155/6400 [==================>...........] - ETA: 5:35 - loss: 11.2844 - acc: 0.292299134\n",
      "4306/6400 [===================>..........] - ETA: 5:13 - loss: 11.2996 - acc: 0.291298565\n",
      "5062/6400 [======================>.......] - ETA: 3:20 - loss: 11.2938 - acc: 0.291654576\n",
      "5335/6400 [========================>.....] - ETA: 2:39 - loss: 11.2986 - acc: 0.291398569\n",
      "5544/6400 [========================>.....] - ETA: 2:08 - loss: 11.3184 - acc: 0.290017637\n",
      "5785/6400 [==========================>...] - ETA: 1:32 - loss: 11.3264 - acc: 0.289554578\n",
      "6400/6400 [==============================] - 958s 150ms/step - loss: 11.2917 - acc: 0.2917\n",
      "Epoch 4/5\n",
      " 194/6400 [..............................] - ETA: 15:26 - loss: 11.4226 - acc: 0.283517636\n",
      " 391/6400 [>.............................] - ETA: 14:56 - loss: 11.2942 - acc: 0.2916108925\n",
      "1172/6400 [====>.........................] - ETA: 13:01 - loss: 11.2358 - acc: 0.295299134\n",
      "1441/6400 [=====>........................] - ETA: 12:21 - loss: 11.3400 - acc: 0.288717637\n",
      "1535/6400 [======>.......................] - ETA: 12:06 - loss: 11.3934 - acc: 0.285398569\n",
      "1734/6400 [=======>......................] - ETA: 11:37 - loss: 11.3914 - acc: 0.285517631\n",
      "1799/6400 [=======>......................] - ETA: 11:27 - loss: 11.3697 - acc: 0.286898565\n",
      "2488/6400 [==========>...................] - ETA: 9:44 - loss: 11.4954 - acc: 0.2789133297\n",
      "2755/6400 [===========>..................] - ETA: 9:04 - loss: 11.4693 - acc: 0.280698567\n",
      "4217/6400 [==================>...........] - ETA: 5:25 - loss: 11.3453 - acc: 0.288417634\n",
      "4843/6400 [=====================>........] - ETA: 3:52 - loss: 11.2943 - acc: 0.291655783\n",
      "5313/6400 [=======================>......] - ETA: 2:42 - loss: 11.2464 - acc: 0.294654576\n",
      "5378/6400 [========================>.....] - ETA: 2:32 - loss: 11.2498 - acc: 0.294317635\n",
      "5411/6400 [========================>.....] - ETA: 2:27 - loss: 11.2489 - acc: 0.294454578\n",
      "6400/6400 [==============================] - 955s 149ms/step - loss: 11.2917 - acc: 0.2917\n",
      "Epoch 5/5\n",
      " 904/6400 [===>..........................] - ETA: 13:40 - loss: 11.4630 - acc: 0.281055783\n",
      "1459/6400 [=====>........................] - ETA: 12:18 - loss: 11.2657 - acc: 0.293454578\n",
      "1609/6400 [======>.......................] - ETA: 11:55 - loss: 11.2855 - acc: 0.292154576\n",
      "2038/6400 [========>.....................] - ETA: 10:51 - loss: 11.3349 - acc: 0.289017636\n",
      "2252/6400 [=========>....................] - ETA: 10:19 - loss: 11.3267 - acc: 0.289517634\n",
      "2733/6400 [===========>..................] - ETA: 9:07 - loss: 11.2524 - acc: 0.2942133297\n",
      "3415/6400 [===============>..............] - ETA: 7:25 - loss: 11.2787 - acc: 0.292517637\n",
      "3573/6400 [===============>..............] - ETA: 7:01 - loss: 11.2842 - acc: 0.292298567\n",
      "3657/6400 [================>.............] - ETA: 6:49 - loss: 11.2822 - acc: 0.292317635\n",
      "4887/6400 [=====================>........] - ETA: 3:45 - loss: 11.3655 - acc: 0.287198565\n",
      "5002/6400 [======================>.......] - ETA: 3:28 - loss: 11.3719 - acc: 0.2867108925\n",
      "5204/6400 [=======================>......] - ETA: 2:58 - loss: 11.3441 - acc: 0.288417631\n",
      "5724/6400 [=========================>....] - ETA: 1:40 - loss: 11.3051 - acc: 0.290999134\n",
      "5766/6400 [==========================>...] - ETA: 1:34 - loss: 11.2891 - acc: 0.291998569\n",
      "6400/6400 [==============================] - 954s 149ms/step - loss: 11.2867 - acc: 0.2920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.299165204912438, 0.29125000000000001]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = utils.FfmpegLoader(sampling_rate=16000)\n",
    "#loader = utils.LibrosaLoader(sampling_rate=16000)\n",
    "SampleLoader = utils.build_sample_loader(AUDIO_DIR, listens_label, loader)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Reshape((-1, 1), input_shape=loader.shape))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Conv1D(128, 512, strides=512))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv1D(32, 8))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(4))\n",
    "\n",
    "model.add(Conv1D(32, 8))\n",
    "print(model.output_shape)\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(4))\n",
    "\n",
    "print(model.output_shape)\n",
    "# model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "model.add(Dense(100))\n",
    "model.add(Activation(\"relu\"))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(listens_label.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "#optimizer = keras.optimizers.Adam()#lr=1e-5)#, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit_generator(SampleLoader(train, batch_size=1), train.size, epochs=5)\n",
    "    loss = model.evaluate_generator(SampleLoader(val, batch_size=1), val.size)\n",
    "    loss = model.evaluate_generator(SampleLoader(test, batch_size=1), test.size)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
